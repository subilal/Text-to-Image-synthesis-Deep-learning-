{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import tensorlayer as tl\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorlayer.layers import *\n",
    "from tensorlayer.prepro import *\n",
    "from tensorlayer.cost import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"1 : cwd \" + cwd)\n",
    "img_dir = os.path.join(cwd, 'CUB\\\\images')\n",
    "print(\"1 : img_dir \" + img_dir)\n",
    "\n",
    "caption_dir = os.path.join(cwd, 'birds\\\\text_c10')\n",
    "print(\"1 : caption_dir \" + caption_dir)\n",
    "\n",
    "VOC_FIR = cwd + '\\\\vocab.txt'\n",
    "print(\"1 : VOC_FIR \" + VOC_FIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load captions\n",
    "caption_sub_dir = load_folder_list( caption_dir )\n",
    "caption_sub_dir\n",
    "captions_dict = {}\n",
    "processed_capts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_dir in caption_sub_dir: ### EDIT\n",
    "    for file in os.listdir(sub_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_dir =os.path.join(sub_dir, file)\n",
    "            print(file_dir)\n",
    "            file_num = int(file[1:6])\n",
    "            \n",
    "            print(file_num)\n",
    "            Open_File = open(file_dir,'r')\n",
    "            Buffer_Line = []\n",
    "            for Line in Open_File:\n",
    "                Line = preprocess_caption(Line)\n",
    "                Buffer_Line.append(Line)\n",
    "                words = nltk.word_tokenize(Line)\n",
    "                words.insert(0,\"<S>\")\n",
    "                words.append(\"</S>\")\n",
    "                processed_capts.append(words)\n",
    "            assert len(Buffer_Line) == 10, \"Every flower image have 10 captions\"\n",
    "            captions_dict[file_num] = Buffer_Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    print(\"  [TL] Creating vocabulary.\")\n",
    "    counter = Counter()\n",
    "    for c in processed_capts:\n",
    "        counter.update(c)\n",
    "        print('c',c)\n",
    "    print(\"    Total words: %d\" % len(counter))\n",
    "\n",
    "    # Filter uncommon words and sort by descending count.\n",
    "    word_counts = [x for x in counter.items() if x[1] >= 1]\n",
    "    word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    word_counts = [(\"<PAD>\", 0)] + word_counts # 1st id should be reserved for padding\n",
    "    # print(word_counts)\n",
    "    print(\"    Words in vocabulary: %d\" % len(word_counts))\n",
    "\n",
    "    if os.path.exists(VOC_FIR):\n",
    "          os.remove(VOC_FIR)\n",
    "    f = open(VOC_FIR, \"w\")\n",
    "    f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word_counts]))\n",
    "    f.close()\n",
    "    print(\"    Wrote vocabulary file: %s\" % VOC_FIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    reverse_vocab = [x[0] for x in word_counts]\n",
    "    unk_id = len(reverse_vocab)\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    print( vocab['color'])\n",
    "    \n",
    "    captions_ids = []\n",
    "    tmp = captions_dict.items()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for key, value in tmp:\n",
    "        for v in value:\n",
    "            captions_ids.append( [vocab[word] for word in nltk.tokenize.word_tokenize(v)] + [vocab['</S>']])  # add END_ID\n",
    "    captions_ids = np.asarray(captions_ids)\n",
    "    print(\" * tokenized %d captions\" % len(captions_ids))\n",
    "    print(captions_ids)\n",
    "    \n",
    "    \n",
    "    img_capt = captions_dict[1][1]\n",
    "    print(\"img_capt: %s\" % img_capt)\n",
    "    print(\"nltk.tokenize.word_tokenize(img_capt): %s\" % nltk.tokenize.word_tokenize(img_capt))\n",
    "    img_capt_ids = [vocab[word] for word in nltk.tokenize.word_tokenize(img_capt)]#img_capt.split(' ')]\n",
    "    print(\"img_capt_ids: %s\" % img_capt_ids)\n",
    "    print(\"id_to_word: %s\" % [list(vocab.keys())[list(vocab.values()).index(id)] for id in img_capt_ids])\n",
    "\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from PIL import Image  \n",
    "\n",
    "s = time.time()\n",
    "images = []\n",
    "images_256 = []\n",
    "\n",
    "return_list = []\n",
    "print(img_dir)\n",
    "image_sub_dir = load_folder_list( img_dir )\n",
    "for sub_dir in image_sub_dir:\n",
    "    for file in os.listdir(sub_dir):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            file_dir =os.path.join(img_dir, sub_dir, file)\n",
    "            print(file_dir)\n",
    "            img_raw = Image.open( file_dir ).convert('RGB')\n",
    "            \n",
    "            img_raw = img_raw.resize( (64, 64))\n",
    "            img_raw_256 = img_raw.resize( (256, 256))\n",
    "            #img_raw = img_raw.astype(np.float32)\n",
    "            img_raw = np.asarray(img_raw)\n",
    "            img_raw_256 = np.asarray(img_raw_256)\n",
    "            images.append(img_raw)\n",
    "            images_256.append(img_raw_256)\n",
    "\n",
    "    print(\" * loading and resizing took %ss\" % (time.time()-s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "n_images = len(captions_dict)\n",
    "n_captions = len(captions_ids)\n",
    "n_captions_per_image = len(Buffer_Line) # 10\n",
    "\n",
    "print(\"n_captions: %d n_images: %d n_captions_per_image: %d\" % (n_captions, n_images, n_captions_per_image))\n",
    "\n",
    "captions_ids_train, captions_ids_test = captions_ids[: 8000*n_captions_per_image], captions_ids[8000*n_captions_per_image :]\n",
    "images_train, images_test = images[:8000], images[8000:]\n",
    "images_train_256, images_test_256 = images_256[:8000], images_256[8000:]\n",
    "n_images_train = len(images_train)\n",
    "n_images_test = len(images_test)\n",
    "n_captions_train = len(captions_ids_train)\n",
    "n_captions_test = len(captions_ids_test)\n",
    "print(\"n_images_train:%d n_captions_train:%d\" % (n_images_train, n_captions_train))\n",
    "print(\"n_images_test:%d  n_captions_test:%d\" % (n_images_test, n_captions_test))        \n",
    "\n",
    "def save_all(targets, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(targets, f)\n",
    "\n",
    "save_all(vocab, '_vocab.pickle')\n",
    "save_all((images_train_256, images_train), '_image_train.pickle')\n",
    "save_all((images_test_256, images_test), '_image_test.pickle')\n",
    "save_all((n_captions_train, n_captions_test, n_captions_per_image, n_images_train, n_images_test), '_n.pickle')\n",
    "save_all((captions_ids_train, captions_ids_test), '_caption.pickle')\n",
    "\n",
    "print(\"images_train\",len(images_train))\n",
    "print(\"images_test\",len(images_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 64\n",
    "z_dim = 512         \n",
    "image_size = 64    \n",
    "c_dim = 3          \n",
    "lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.2)\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"_vocab.pickle\", 'rb') as f:\n",
    "     vocab = pickle.load(f)\n",
    "with open(\"_image_train.pickle\", 'rb') as f:\n",
    "     _, images_train = pickle.load(f)\n",
    "with open(\"_image_test.pickle\", 'rb') as f:\n",
    "     _, images_test = pickle.load(f)\n",
    "with open(\"_n.pickle\", 'rb') as f:\n",
    "     n_captions_train, n_captions_test, n_captions_per_image, n_images_train, n_images_test = pickle.load(f)\n",
    "with open(\"_caption.pickle\", 'rb') as f:\n",
    "     captions_ids_train, captions_ids_test = pickle.load(f)\n",
    "\n",
    "        \n",
    "images_train = np.array(images_train).astype(np.float32)\n",
    "images_test = np.array(images_test)\n",
    "ni = int(np.ceil(np.sqrt(batch_size)))\n",
    "\n",
    "if not os.path.exists(\"samples/step1_gan-cls\"):\n",
    "    os.makedirs(\"samples/step1_gan-cls\")\n",
    "if not os.path.exists(\"samples/step_pretrain_encoder\"):\n",
    "    os.makedirs(\"samples/step_pretrain_encoder\")        \n",
    "if not os.path.exists(\"checkpoint\"):\n",
    "    os.makedirs(\"checkpoint\")\n",
    "save_dir = \"checkpoint\"\n",
    "\n",
    "\n",
    "w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "df_dim = 64\n",
    "inputs = []\n",
    "## for text-to-image mapping ===================================================\n",
    "t_dim = 128         # text feature dimension\n",
    "rnn_hidden_size = t_dim\n",
    "vocab_size = 8000\n",
    "word_embedding_size = 256\n",
    "keep_prob = 1.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rnn_embed(input_seqs, is_train=True, reuse=False, return_embed=False):\n",
    "    \"\"\" txt --> t_dim \"\"\"\n",
    "    w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    if tf.__version__ <= '0.12.1':\n",
    "        LSTMCell = tf.nn.rnn_cell.LSTMCell\n",
    "    else:\n",
    "        LSTMCell = tf.contrib.rnn.BasicLSTMCell\n",
    "    with tf.variable_scope(\"rnnftxt\", reuse=tf.AUTO_REUSE):\n",
    "        tl.layers.set_name_reuse(True)\n",
    "        network = EmbeddingInputlayer(\n",
    "                     inputs = input_seqs,\n",
    "                     vocabulary_size = vocab_size,\n",
    "                     embedding_size = word_embedding_size,\n",
    "                     E_init = w_init,\n",
    "                     name = 'rnn/wordembed')\n",
    "        network = DynamicRNNLayer(network,\n",
    "                     cell_fn = LSTMCell,\n",
    "                     cell_init_args = {'state_is_tuple' : True, 'reuse': reuse},  # for TF1.1, TF1.2 dont need to set reuse\n",
    "                     n_hidden = rnn_hidden_size,\n",
    "                     dropout = (keep_prob if is_train else None),\n",
    "                     initializer = w_init,\n",
    "                     sequence_length = tl.layers.retrieve_seq_length_op2(input_seqs),\n",
    "                     return_last = True,\n",
    "                     name = 'rnn/dynamic')\n",
    "        return network\n",
    "\n",
    "    \n",
    "\n",
    "def cnn_encoder(inputs, is_train=True, reuse=False, name='cnnftxt', return_h3=False):\n",
    "    \"\"\" 64x64 --> t_dim, for text-image mapping \"\"\"\n",
    "    w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "    df_dim = 64\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "        tl.layers.set_name_reuse(tf.AUTO_REUSE)\n",
    "\n",
    "        net_in = InputLayer(inputs, name='/in')\n",
    "        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                padding='SAME', W_init=w_init, name='cnnf/h0/conv2d')\n",
    "\n",
    "        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='cnnf/h1/conv2d')\n",
    "        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='cnnf/h1/batch_norm')\n",
    "\n",
    "        # if name != 'cnn': # debug for training image encoder in step 2\n",
    "        #     net_h1 = DropoutLayer(net_h1, keep=0.8, is_fix=True, name='p/h1/drop')\n",
    "\n",
    "        net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='cnnf/h2/conv2d')\n",
    "        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='cnnf/h2/batch_norm')\n",
    "\n",
    "        # if name != 'cnn': # debug for training image encoder in step 2\n",
    "        #     net_h2 = DropoutLayer(net_h2, keep=0.8, is_fix=True, name='p/h2/drop')\n",
    "\n",
    "        net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='cnnf/h3/conv2d')\n",
    "        net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='cnnf/h3/batch_norm')\n",
    "\n",
    "        # if name != 'cnn': # debug for training image encoder in step 2\n",
    "        #     net_h3 = DropoutLayer(net_h3, keep=0.8, is_fix=True, name='p/h3/drop')\n",
    "\n",
    "        net_h4 = FlattenLayer(net_h3, name='cnnf/h4/flatten')\n",
    "        net_h4 = DenseLayer(net_h4, n_units= (z_dim if name == 'z_encoder' else t_dim),\n",
    "                act=tf.identity,\n",
    "                W_init = w_init, b_init = None, name='cnnf/h4/embed')\n",
    "    if return_h3:\n",
    "        return net_h4, net_h3\n",
    "    else:\n",
    "        return net_h4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator_txt2img_resnet(input_z, t_txt=None, is_train=True, reuse=False, batch_size=batch_size):\n",
    "    \"\"\" z + (txt) --> 64x64 \"\"\"\n",
    "    # https://github.com/hanzhanggit/StackGAN/blob/master/stageI/model.py\n",
    "    s = image_size # output image size [64]\n",
    "    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n",
    "    gf_dim = 128\n",
    "\n",
    "    w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "\n",
    "    with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        tl.layers.set_name_reuse(True)\n",
    "        net_in = InputLayer(input_z, name='g_inputz')\n",
    "\n",
    "        if t_txt is not None:\n",
    "            net_txt = InputLayer(t_txt, name='g_input_txt')\n",
    "            net_txt = DenseLayer(net_txt, n_units=t_dim,\n",
    "                act=lambda x: tl.act.lrelu(x, 0.2), W_init=w_init, name='g_reduce_text/dense')\n",
    "            net_in = ConcatLayer([net_in, net_txt], concat_dim=1, name='g_concat_z_txt')\n",
    "\n",
    "        net_h0 = DenseLayer(net_in, gf_dim*8*s16*s16, act=tf.identity,\n",
    "                W_init=w_init, b_init=None, name='g_h0/dense')\n",
    "        net_h0 = BatchNormLayer(net_h0,  #act=tf.nn.relu,\n",
    "                is_train=is_train, gamma_init=gamma_init, name='g_h0/batch_norm')\n",
    "        net_h0 = ReshapeLayer(net_h0, [-1, s16, s16, gf_dim*8], name='g_h0/reshape')\n",
    "\n",
    "        net = Conv2d(net_h0, gf_dim*2, (1, 1), (1, 1),\n",
    "                padding='VALID', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d')\n",
    "        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n",
    "                gamma_init=gamma_init, name='g_h1_res/batch_norm')\n",
    "        net = Conv2d(net, gf_dim*2, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d2')\n",
    "        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n",
    "                gamma_init=gamma_init, name='g_h1_res/batch_norm2')\n",
    "        net = Conv2d(net, gf_dim*8, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d3')\n",
    "        net = BatchNormLayer(net, # act=tf.nn.relu,\n",
    "                is_train=is_train, gamma_init=gamma_init, name='g_h1_res/batch_norm3')\n",
    "        net_h1 = ElementwiseLayer(layer=[net_h0, net], combine_fn=tf.add, name='g_h1_res/add')\n",
    "        net_h1.outputs = tf.nn.relu(net_h1.outputs)\n",
    "\n",
    "        # Note: you can also use DeConv2d to replace UpSampling2dLayer and Conv2d\n",
    "        # net_h2 = DeConv2d(net_h1, gf_dim*4, (4, 4), out_size=(s8, s8), strides=(2, 2),\n",
    "        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h2/decon2d')\n",
    "        net_h2 = UpSampling2dLayer(net_h1, size=[s8, s8], is_scale=False, method=1,\n",
    "                align_corners=False, name='g_h2/upsample2d')\n",
    "        net_h2 = Conv2d(net_h2, gf_dim*4, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h2/conv2d')\n",
    "        net_h2 = BatchNormLayer(net_h2,# act=tf.nn.relu,\n",
    "                is_train=is_train, gamma_init=gamma_init, name='g_h2/batch_norm')\n",
    "\n",
    "        net = Conv2d(net_h2, gf_dim, (1, 1), (1, 1),\n",
    "                padding='VALID', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d')\n",
    "        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n",
    "                gamma_init=gamma_init, name='g_h3_res/batch_norm')\n",
    "        net = Conv2d(net, gf_dim, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d2')\n",
    "        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n",
    "                gamma_init=gamma_init, name='g_h3_res/batch_norm2')\n",
    "        net = Conv2d(net, gf_dim*4, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d3')\n",
    "        net = BatchNormLayer(net, #act=tf.nn.relu,\n",
    "                is_train=is_train, gamma_init=gamma_init, name='g_h3_res/batch_norm3')\n",
    "        net_h3 = ElementwiseLayer(layer=[net_h2, net], combine_fn=tf.add, name='g_h3/add')\n",
    "        net_h3.outputs = tf.nn.relu(net_h3.outputs)\n",
    "\n",
    "        # net_h4 = DeConv2d(net_h3, gf_dim*2, (4, 4), out_size=(s4, s4), strides=(2, 2),\n",
    "        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h4/decon2d'),\n",
    "        net_h4 = UpSampling2dLayer(net_h3, size=[s4, s4], is_scale=False, method=1,\n",
    "                align_corners=False, name='g_h4/upsample2d')\n",
    "        net_h4 = Conv2d(net_h4, gf_dim*2, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h4/conv2d')\n",
    "        net_h4 = BatchNormLayer(net_h4, act=tf.nn.relu,\n",
    "                is_train=is_train, gamma_init=gamma_init, name='g_h4/batch_norm')\n",
    "\n",
    "        # net_h5 = DeConv2d(net_h4, gf_dim, (4, 4), out_size=(s2, s2), strides=(2, 2),\n",
    "        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h5/decon2d')\n",
    "        net_h5 = UpSampling2dLayer(net_h4, size=[s2, s2], is_scale=False, method=1,\n",
    "                align_corners=False, name='g_h5/upsample2d')\n",
    "        net_h5 = Conv2d(net_h5, gf_dim, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h5/conv2d')\n",
    "        net_h5 = BatchNormLayer(net_h5, act=tf.nn.relu,\n",
    "                is_train=is_train, gamma_init=gamma_init, name='g_h5/batch_norm')\n",
    "\n",
    "        # net_ho = DeConv2d(net_h5, c_dim, (4, 4), out_size=(s, s), strides=(2, 2),\n",
    "        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_ho/decon2d')\n",
    "        net_ho = UpSampling2dLayer(net_h5, size=[s, s], is_scale=False, method=1,\n",
    "                align_corners=False, name='g_ho/upsample2d')\n",
    "        net_ho = Conv2d(net_ho, c_dim, (3, 3), (1, 1),\n",
    "                padding='SAME', act=None, W_init=w_init, name='g_ho/conv2d')\n",
    "        logits = net_ho.outputs\n",
    "        net_ho.outputs = tf.nn.tanh(net_ho.outputs)\n",
    "    return net_ho, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discriminator_txt2img_resnet(input_images, t_txt=None, is_train=True, reuse=False):\n",
    "    \"\"\" 64x64 + (txt) --> real/fake \"\"\"\n",
    "    # https://github.com/hanzhanggit/StackGAN/blob/master/stageI/model.py\n",
    "    # Discriminator with ResNet : line 197 https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n",
    "    w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    gamma_init=tf.random_normal_initializer(1., 0.02)\n",
    "    df_dim = 64  # 64 for flower, 196 for MSCOCO\n",
    "    s = 64 # output image size [64]\n",
    "    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        tl.layers.set_name_reuse(True)\n",
    "        net_in = InputLayer(input_images, name='d_input/images')\n",
    "        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                padding='SAME', W_init=w_init, name='d_h0/conv2d')\n",
    "\n",
    "        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='d_h1/conv2d')\n",
    "        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='d_h1/batchnorm')\n",
    "        net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='d_h2/conv2d')\n",
    "        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='d_h2/batchnorm')\n",
    "        net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='d_h3/conv2d')\n",
    "        net_h3 = BatchNormLayer(net_h3, #act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm')\n",
    "\n",
    "        net = Conv2d(net_h3, df_dim*2, (1, 1), (1, 1), act=None,\n",
    "                padding='VALID', W_init=w_init, b_init=None, name='d_h4_res/conv2d')\n",
    "        net = BatchNormLayer(net, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm')\n",
    "        net = Conv2d(net, df_dim*2, (3, 3), (1, 1), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='d_h4_res/conv2d2')\n",
    "        net = BatchNormLayer(net, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm2')\n",
    "        net = Conv2d(net, df_dim*8, (3, 3), (1, 1), act=None,\n",
    "                padding='SAME', W_init=w_init, b_init=None, name='d_h4_res/conv2d3')\n",
    "        net = BatchNormLayer(net, #act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm3')\n",
    "        net_h4 = ElementwiseLayer(layer=[net_h3, net], combine_fn=tf.add, name='d_h4/add')\n",
    "        net_h4.outputs = tl.act.lrelu(net_h4.outputs, 0.2)\n",
    "\n",
    "        if t_txt is not None:\n",
    "            net_txt = InputLayer(t_txt, name='d_input_txt')\n",
    "            net_txt = DenseLayer(net_txt, n_units=t_dim,\n",
    "                   act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                   W_init=w_init, name='d_reduce_txt/dense')\n",
    "            net_txt = ExpandDimsLayer(net_txt, 1, name='d_txt/expanddim1')\n",
    "            net_txt = ExpandDimsLayer(net_txt, 1, name='d_txt/expanddim2')\n",
    "            net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='d_txt/tile')\n",
    "            net_h4_concat = ConcatLayer([net_h4, net_txt], concat_dim=3, name='d_h3_concat')\n",
    "            # 243 (ndf*8 + 128 or 256) x 4 x 4\n",
    "            net_h4 = Conv2d(net_h4_concat, df_dim*8, (1, 1), (1, 1),\n",
    "                    padding='VALID', W_init=w_init, b_init=None, name='d_h3/conv2d_2')\n",
    "            net_h4 = BatchNormLayer(net_h4, act=lambda x: tl.act.lrelu(x, 0.2),\n",
    "                    is_train=is_train, gamma_init=gamma_init, name='d_h3/batch_norm_2')\n",
    "\n",
    "        net_ho = Conv2d(net_h4, 1, (s16, s16), (s16, s16), padding='VALID', W_init=w_init, name='d_ho/conv2d')\n",
    "        # 1 x 1 x 1\n",
    "        # net_ho = FlattenLayer(net_h4, name='d_ho/flatten')\n",
    "        logits = net_ho.outputs\n",
    "        net_ho.outputs = tf.nn.sigmoid(net_ho.outputs)\n",
    "    return net_ho, logits        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_real_image = tf.placeholder('float32', [batch_size, image_size, image_size, 3], name = 'real_image')\n",
    "t_wrong_image = tf.placeholder('float32', [batch_size ,image_size, image_size, 3], name = 'wrong_image')\n",
    "t_real_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='real_caption_input')\n",
    "t_wrong_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='wrong_caption_input')\n",
    "t_z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z_noise')\n",
    "\n",
    "## training inference for text-to-image mapping\n",
    "net_cnn = cnn_encoder(t_real_image, is_train=True, reuse=False)\n",
    "x = net_cnn.outputs\n",
    "v = rnn_embed(t_real_caption, is_train=True, reuse=False).outputs\n",
    "x_w = cnn_encoder(t_wrong_image, is_train=True, reuse=True).outputs\n",
    "v_w = rnn_embed(t_wrong_caption, is_train=True, reuse=True).outputs\n",
    "\n",
    "alpha = 0.2 # margin alpha\n",
    "rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n",
    "            tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))\n",
    "\n",
    "## training inference for txt2img\n",
    "generator_txt2img = generator_txt2img_resnet\n",
    "discriminator_txt2img = discriminator_txt2img_resnet\n",
    "\n",
    "net_rnn = rnn_embed(t_real_caption, is_train=False, reuse=True)\n",
    "net_fake_image, _ = generator_txt2img(t_z,\n",
    "                net_rnn.outputs,\n",
    "                is_train=True, reuse=False, batch_size=batch_size)\n",
    "                #+ tf.random_normal(shape=net_rnn.outputs.get_shape(), mean=0, stddev=0.02), # NOISE ON RNN\n",
    "net_d, disc_fake_image_logits = discriminator_txt2img(\n",
    "                net_fake_image.outputs, net_rnn.outputs, is_train=True, reuse=False)\n",
    "_, disc_real_image_logits = discriminator_txt2img(\n",
    "                t_real_image, net_rnn.outputs, is_train=True, reuse=True)\n",
    "_, disc_mismatch_logits = discriminator_txt2img(\n",
    "                # t_wrong_image,\n",
    "                t_real_image,\n",
    "                # net_rnn.outputs,\n",
    "                rnn_embed(t_wrong_caption, is_train=False, reuse=True).outputs,\n",
    "                is_train=True, reuse=True)\n",
    "\n",
    "## testing inference for txt2img\n",
    "net_g, _ = generator_txt2img(t_z,\n",
    "                rnn_embed(t_real_caption, is_train=False, reuse=True).outputs,\n",
    "                is_train=False, reuse=True, batch_size=batch_size)\n",
    "\n",
    "d_loss1 = tl.cost.sigmoid_cross_entropy(disc_real_image_logits, tf.ones_like(disc_real_image_logits), name='d1')\n",
    "d_loss2 = tl.cost.sigmoid_cross_entropy(disc_mismatch_logits,  tf.zeros_like(disc_mismatch_logits), name='d2')\n",
    "d_loss3 = tl.cost.sigmoid_cross_entropy(disc_fake_image_logits, tf.zeros_like(disc_fake_image_logits), name='d3')\n",
    "d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n",
    "g_loss = tl.cost.sigmoid_cross_entropy(disc_fake_image_logits, tf.ones_like(disc_fake_image_logits), name='g')\n",
    "\n",
    "print(\"Stage 1 sucesss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    lr = 0.0002\n",
    "    lr_decay = 0.5      # decay factor for adam, https://github.com/reedscot/icml2016/blob/master/main_cls_int.lua  https://github.com/reedscot/icml2016/blob/master/scripts/train_flowers.sh\n",
    "    decay_every = 100   # https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n",
    "    beta1 = 0.5\n",
    "\n",
    "    cnn_vars = tl.layers.get_variables_with_name('cnn', True, True)\n",
    "    rnn_vars = tl.layers.get_variables_with_name('rnn', True, True)\n",
    "    d_vars = tl.layers.get_variables_with_name('discriminator', True, True)\n",
    "    g_vars = tl.layers.get_variables_with_name('generator', True, True)\n",
    "\n",
    "    with tf.variable_scope('learning_rate'):\n",
    "        lr_v = tf.Variable(lr, trainable=False)\n",
    "        d_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(d_loss, var_list=d_vars )\n",
    "        g_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(g_loss, var_list=g_vars )\n",
    "        # e_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(e_loss, var_list=e_vars + c_vars)\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(rnn_loss, rnn_vars + cnn_vars), 10)\n",
    "        optimizer = tf.train.AdamOptimizer(lr_v, beta1=beta1)# optimizer = tf.train.GradientDescentOptimizer(lre)\n",
    "        rnn_optim = optimizer.apply_gradients(zip(grads, rnn_vars + cnn_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    ## seed for generation, z and sentence ids\n",
    "    sample_size = batch_size\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n",
    "        # sample_seed = np.random.uniform(low=-1, high=1, size=(sample_size, z_dim)).astype(np.float32)]\n",
    "    n = int(sample_size / ni)\n",
    "    sample_sentence = [\"this bird has a bright yellow body, with brown on its crown and wings.\"] * n + \\\n",
    "                      [\"this bird has a red breast and belly as well as a small bill.\"] * n + \\\n",
    "                      [\"small, roundish bird with off white breast and belly, light brown crown, brown and black colored wings.\"] * n + \\\n",
    "                      [\"a white bird with a black crown and yellow beak\"] * n + \\\n",
    "                      [\"the bird has gray crown, belly and white abdomen, with black tarsus and feet\"] * n + \\\n",
    "                      [\"a colorful bird with a bright yellow body, a black crown and throat, orange bill, and black primaries and secondaries.\"] * n + \\\n",
    "                      [\"this bird has wings that are blue and white.\"] * n +\\\n",
    "                      [\"these white bird have wings off white in color and end in a white towards the tips.\"] * n\n",
    "\n",
    "    # sample_sentence = captions_ids_test[0:sample_size]\n",
    "    for i, sentence in enumerate(sample_sentence):\n",
    "        print(\"seed: %s\" % sentence)\n",
    "        sentence = preprocess_caption(sentence)\n",
    "        sample_sentence[i] = [vocab[word] for word in nltk.tokenize.word_tokenize(sentence)] + [vocab['</S>']]    # add END_ID\n",
    "        # sample_sentence[i] = [vocab.word_to_id(word) for word in sentence]\n",
    "        # print(sample_sentence[i])\n",
    "    sample_sentence = tl.prepro.pad_sequences(sample_sentence, padding='post')\n",
    "\n",
    "    n_epoch = 600\n",
    "    print_freq = 1\n",
    "    n_batch_epoch = int(n_images_train / batch_size)\n",
    "    print(\"n_batch_epoch\",  n_batch_epoch)\n",
    "    \n",
    "    # exit()\n",
    "    for epoch in range(0, n_epoch+1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        if epoch !=0 and (epoch % decay_every == 0):\n",
    "            new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "            sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
    "            log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "            print(log)\n",
    "            # logging.debug(log)\n",
    "        elif epoch == 0:\n",
    "            log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "            print(log)\n",
    "\n",
    "        for step in range(n_batch_epoch):\n",
    "            step_time = time.time()\n",
    "            ## get matched text\n",
    "            idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "            b_real_caption = captions_ids_train[idexs]\n",
    "            b_real_caption = tl.prepro.pad_sequences(b_real_caption, padding='post')\n",
    "            ## get real image\n",
    "            b_real_images = images_train[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n",
    "            # save_images(b_real_images, [ni, ni], 'samples/step1_gan-cls/train_00.png')\n",
    "            ## get wrong caption\n",
    "            idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "            b_wrong_caption = captions_ids_train[idexs]\n",
    "            b_wrong_caption = tl.prepro.pad_sequences(b_wrong_caption, padding='post')\n",
    "            ## get wrong image\n",
    "            idexs2 = get_random_int(min=0, max=n_images_train-1, number=batch_size)\n",
    "            b_wrong_images = images_train[idexs2]\n",
    "            ## get noise\n",
    "            b_z = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n",
    "                # b_z = np.random.uniform(low=-1, high=1, size=[batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "            b_real_images = threading_data(b_real_images, prepro_img, mode='train')   # [0, 255] --> [-1, 1] + augmentation\n",
    "            b_wrong_images = threading_data(b_wrong_images, prepro_img, mode='train')\n",
    "            ## updates text-to-image mapping\n",
    "            if epoch < 50:\n",
    "                errRNN, _ = sess.run([rnn_loss, rnn_optim], feed_dict={\n",
    "                                                t_real_image : b_real_images,\n",
    "                                                t_wrong_image : b_wrong_images,\n",
    "                                                t_real_caption : b_real_caption,\n",
    "                                                t_wrong_caption : b_wrong_caption})\n",
    "            else:\n",
    "                errRNN = 0\n",
    "\n",
    "            ## updates D\n",
    "            errD, _ = sess.run([d_loss, d_optim], feed_dict={\n",
    "                            t_real_image : b_real_images,\n",
    "                            # t_wrong_image : b_wrong_images,\n",
    "                            t_wrong_caption : b_wrong_caption,\n",
    "                            t_real_caption : b_real_caption,\n",
    "                            t_z : b_z})\n",
    "            ## updates G\n",
    "            errG, _ = sess.run([g_loss, g_optim], feed_dict={\n",
    "                            t_real_caption : b_real_caption,\n",
    "                            t_z : b_z})\n",
    "\n",
    "            print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.8f, g_loss: %.8f, rnn_loss: %.8f\" \\\n",
    "                        % (epoch, n_epoch, step, n_batch_epoch, time.time() - step_time, errD, errG, errRNN))\n",
    "\n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print(\" ** Epoch %d took %fs\" % (epoch, time.time()-start_time))\n",
    "            img_gen, rnn_out = sess.run([net_g.outputs, net_rnn.outputs], feed_dict={\n",
    "                                        t_real_caption : sample_sentence,\n",
    "                                        t_z : sample_seed})\n",
    "\n",
    "            # img_gen = threading_data(img_gen, prepro_img, mode='rescale')\n",
    "            save_images(img_gen, [ni, ni], 'samples/step1_gan-cls/train_{:02d}.png'.format(epoch))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
